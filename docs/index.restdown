---
title: Manatee
markdown2extras: wiki-tables, code-friendly
apisections:
---

# Manatee - Postgres HA Cluster.

This document describes the high level design for Manatee - A system built on
top of Postgres synchronous replication to deliver a Postgres cluster that is
consistent and partition tolerant whilst maxmimizing availability.

# Background

It's assumed readers are familiar with Zookeeper and Postgres. Read through
[this](http://zookeeper.apache.org/doc/trunk/zookeeperOver.html) Zookeeper guide
before starting this document.

Postgres offers synchronous and asynchronous streaming replication. For a
detailed look, check out the Postgres [docs](http://www.Postgresql.org/docs/9.1/interactive/warm-standby.html#SYNCHRONOUS-REPLICATION) on this subject matter.

## Brief Overview of Postgres Synchronous Replication

In this scheme, there are 2 Postgres peers, a primary and a standby. The standby
operates in read-only mode whilst the primary takes all of the writes. Each
commit of a write transaction will wait until confirmation is received that the
commit has been written to the transaction log of both the primary and the standby.

Responses to the client on writes made to the primary will wait until the
standby responds. The response may never occur if the standby crashes.
Regardless of whether the standby responds, the record will be written to the
primary. From the point of view of the client, the response is
indistinguishable from a 500, and the client will not know for certain whether
the write request has succeeded.

Basically, the default Postgres implementation of synchronous replication results
in inconsistency between the client and Postgres on standby failures. Postgres'
solution to this is achieved by adding multiple potential synchronous standbys.
The first one in the list will be used as the synchronous standby. Standbys
listed after this will take over as the synchronous standby should the first one
fail.

This default setup is insufficient when the primary fails.
Postgres does not provide automatic primary failover, which results in the loss
of write availability until an operator can be engaged.

# Design Goals

A Postgres shard consisting of 3 peers must be able to:

1) Provide group membership discovery to clients.
2) Remain available to writes/reads in the face of failure of any 1 peer - Specifically, re-assigning the role of primary to a standby peer in the face of primary failure.
3) Automate peer recovery - If a peer is down for an amount of time that exceeds the WAL cache, backup/restore from another peer should be automatic.

With these goals in mind, we can ensure Manatee will be available, consistent,
and partition tolerant as long as each shard doesn't lose more than 1 out of its
3 peers.

All bets are off if we lose more than 1. In this case, the shard will remain in
readonly mode until operator intervention.

# Overview

Manatee consists of the following logical components:
- Zookeeper.
- Postgres.
- Backup/Restore agent.
- Postgres Sitter.
- Registrar Service.

# Zookeeper

Zookeeper is used for the following tasks:

-Liveliness checks of Peers within a shard.
-Primary election of Peers within a shard.
-Discovery of Shards by Manatee clients.

Tasks 1 and 2 are described in the Postgres Sitter section, whilst task 3 is described in the Registrar Service section.

# Shards and Peers

A Postgres Shard in Manatee is uniquely identified by a GUID.
The Shard consists of 3 mirrored Postgres instances, henceforth known as Peers.
Shard membership is stored under a configurable persistent znode under zookeeper path:
  /shards/
While each shard entry is stored under the shard path as an emphemeral znode:
  /shards/8d7bda74-7495-4765-80ad-87a1f99ef173
  /shards/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc

Each shard entry contains the following JSON object:
  {
    primary: 'postgres://user@ip:port/db',
    sync: 'postgres://user@ip:port/db',
    async: 'postgres://user@ip:port/db'
  }

The shard entry is updated/created by the current primary in the shard. As group
memberships change, the entry will be updated accordingly.

# Registrar Service

This is the external REST service that allows clients to query for the current status of
all Postgres Shards in Manatee. Note we could have consumers of Manatee directly
query zookeeper, (which would have the benefit of push notifications). However,
the REST service allows us to reduce the load to Zookeeper.

Internally the Registrar service maintains a connection to Zookeeper and updates
the shards when changes occur.

## GET /shards

Returns a list of shard URIs.

## GET /shards/:uuid

Returns a shard entry in the format described in the previous section.

## Clients

Clients should maintain a cache of the list of shards. The cache can be configured
with a TTL. If 500s are received from a particular Postgres shard, e.g. the primary is unavailable, or no longer the primary, then the client should refresh its cache by querying the Registrar Service.

# Restore Service

This is an internal REST service that allows peers in each shard to recover or bootstrap themselves from another peer in the service.
This service is only used when the recovery of a peer from just WAL is not possible. e.g. database corruption, WAL cache exhaustion, or whilst bootstrapping a new peer.
The backup and restore mechanism relies on ZFS snapshots of the db data dir, and utilizes @JWilsdon's ZFS send/recv lib for transport.

This service is made up of the following components on each Postgres Peer:
- A snapshot agent that takes periodic ZFS snapshots of the db dir.
- A REST service that takes backup requests.
- A restore agent that sends the snapshot to the requestor.

The backup process is asynchronous:

1) Client POSTS to /backup/ to indicate a backup request.
2) The service creates /backup/uuid and returns this URI to the client.
3) The service initiates ZFS send/recv to the client.
4) The client polls /backup/uuid for the status of the backup.
5) Once the backup has successfully completed, the service updates /backup/uuid to done status.
6) The client polls /backup/uuid and discovers the backup has finished succesfully.

In practice, backup requests will only be sent to the primary peer of a shard.

# Postgres Sitter

## Postgres Replicaiton Recap

It's important to note that the change of a Postgres' instance's replication state i.e. from standby to primary, or primary to
standby, requires a restart of the Postgres instance.
In addition, the primary Postgres instance requires the URLs of all standby hosts, and each standby host requires the URL of the primary.
The distinction of whether a standby is synchronous or asynchronous is soley made on the primary as the standbys have the same configuration regardess of their replication mode.

## Overview
The postgres sitter is an agent co-located with each Postgres Peer.

The sitter is responsible for:
- Replication role (primary, standby) determination.
- Postgres initialization.
- Start/stop Postgres.
- Restore/bootstrap a corrupt Postgres Peer.
- Postgres health check.
- Primary failover.
- Primary leader election.

The Postgres instance can only be started/stopped by the Sitter, at no time can Postgres start/stop on its own. The Sitter is only changing replication role state of the peers in the event of a primary failure, and not in the event of standby failures. This is described in further detail in the failure section.

The responsibilities of the Sitter is best described via examples.

## Bootstrapping 3 new Peers into a Shard

Let's call the 3 sitters A, B, and C which belong to shard 3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc.
Bootstrapping occurs this way:
- The standard [zookeeper election algorithm](http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection) is run on all 3 sitters.
- The leader becomes primary.
- The remaining two peers become the sync and async standbys, depending on who had the larger sequence.
- The primary initializes its postgres instance.
- The standbys initiate db restoration by calling the Restore service.
- After successful restores, the standbys start their postgers instance.
- The primary checks the status of replication via the pg_stat_replication table.
- If replication has been successfully setup, the primary publishes group membership to the Registrar service. This indicates to clients that this Shard is ready to take writes.

Elaborating on the election process:

Each peer creates an ephemeral||sequence znode under the path /shard/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc/shard- with their postgres url as the data. This results in the following state in Zookeeper:

  /shard/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc/shard-00000 -> {A.url}
  /shard/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc/shard-00001 -> {B.url}
  /shard/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc/shard-00002 -> {C.url}

Each peer then sets a watch on the path /shard/3e051a10-4ab9-4aee-a2bf-0a0efd9c85cc/ and is notified of any changes
to that path. The roles of the Shard is determined by the sequence number of each Peer's znode, in ascending order.
With this example, A becomes the primary, B becomes the synchronous standby, and C becomse the asynchrnous standby.

## Failure of a Single Peer in a Shard.

Each Shard can tolerate failures of any single Peer in the Shard. With the exception of a primary failure, the Sitter does not
actively participate in the failover process. Postgres will automatically promote the asynchronous standby to synchronous should the synchronous standby fail as part of its replication process. The Sitter periodically checks the health status of Postgres along with its session to Zookeeper. If either returns error, the Sitter shuts down postgres and disconnects its session with Zookeeper.

### Primary Fails
The Sitters on both the synchronous and asynchronous standby, due to their watches on the primary's zookeeper znode, will be notified that the primary is offline. Both standbys are also aware of who the synchronous standby by checking with the registrar service. The synchronous standby assumes the role of primary, and the async becomse the synchronous standby. This is only case where there will be a small window of time where writes will return 500s.

### Synchronous Standby Fails

The primary will automatically fail over to the asynchronous standby. The asynchronous standby now assumes the role of the synchronous standby. The Sitter on the primary will update the shard info in zookeeper under /shards/shardid to reflect the new membership of the shard.

### Asynchronous Standby Fails

If the Asynchronous standby is unavailable, no Postgres actions are taken. The Sitter on the primary will update the shard info in Zookeeper to reflect the loss of the asynchronous peer.

## Inserting a new Peer into a Shard

Inserting works with the following algorithm:
- Perform leader election, i.e. set a watch and create an emphemeral-sequence znode under the shard path. If there were any other peers in the shard before you joined, you would not become leader because you would have a higher sequenced znode.
- If you are the leader, then the process is the same as the bootstrap process described earlier.
- If you are not the leader, then join the shard as a standby. Note you do not determine whether you are the async or sync standby, you merely replicate from the primary and the primary will determine your standby role.

